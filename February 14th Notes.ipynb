{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec68e1c",
   "metadata": {},
   "source": [
    "# February 14th Machine Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cf15f",
   "metadata": {},
   "source": [
    "    Everyone is doing quite well on Assignments. \n",
    "       \n",
    "Before talking about the midterm, we will do some \"Housekeeping\".\n",
    "\n",
    "Effectively, people are butthurt that the prof. made the course seem very difficult. He exclaims that the course is not made well by design. Few stats students (who have to take this course) are good at R, but not at Python. Professor was told that students are better at Python, this turned out to be wrong. Furthermore, the actuarial society determines what is necessary by this class. He sympathizes, that is why last year he curved and corrected aspects of the course. Professor will be bringing up to the department that this course, by design, is flawed. He hopes to maybe cover Pandas / Python within a week. Our assignments do not count for pass/fail, but the department requires it must be an exam. He sympathized such that he made the self assessment to be extremely difficult, such that it pushed the class to up their game / catch up.\n",
    "\n",
    "Some of the students expressed distaste that the Prof. posts the videos from the last year. He does this because he would have found those resources highly useful when he was in Undergrad. If that reflects badly on him, he is willing to stop posting them. In this class, he is going to be explaining something that is not in the videos / class from last year.\n",
    "\n",
    "One topic he deems heavily important for this course is accellerated computing. We are playing with \"toy\" datasets in this course, but in the industry, we are given data with millions and millions of rows. That is why we have to use accellerated computing. He sadly does not have enough time to introduce these ideas within the course. He expresses disdain towards the people who decided it cannot be included.\n",
    "\n",
    "A student is expressing difficulty with determining how to get over humps of not knowing particular parts of the assignments, as the forum does not easily allow for solid feedback. Student brings up idea of reviewing the assignment.\n",
    "\n",
    "A student describes difficulty understanding the questions on the assignment, they are worded extremely vaguely. Professor responds by describing how the Forum typically is a solid and useful tool for this. This week is an exception as there are many unanswered questions.\n",
    "\n",
    "Student is concerned about the assignments not translating into the midterm. She reccomends extending the assignment deadline, as they are under extreme pressure to finish the assignment in the given time. Prof will respond to this during the midterm notes\n",
    "\n",
    "Professor expresses that they are new to GGBot (Whatever it is called in the new assignment)\n",
    "\n",
    "# Midterm notes:\n",
    "\n",
    "Prof will think about extending assignment deadline. But, given Assignments are not included in the pass / fail criteria, he does not think it is neccessary, as they are not meant to be exact representations of your mark.\n",
    "\n",
    "For the midterm, this being a data science course, he tends to give an \"end-to-end\" datascience project. Guidances you should follow, such as data exploration, data cleaning, training models, and reporting your metrics. We have already practised part of last terms midterm he says, as it was within the assignment that had to do with the Soccer Stats. \n",
    "\n",
    "He needs to find data sets which are untouched by the internet. He finds them on various platforms.\n",
    "\n",
    "You shold not have to memorize assignments, but you should understand exactly how you solved it. You should also know how to read documentation of what you are working with / working on. The documentations are not like textbooks, they are not clear. They are like rough guidelines which give you sometimes not exactly what you want to know.\n",
    "\n",
    "he tests us on being able to analyze (at a high leveL) the data sets. Have to have a clear idea of what models we could use, and how to evaluate them. To wrap it up, practise the assignments. He will share with us past exams. On the review session he may show us another end-to-end datascience project.\n",
    "\n",
    "Practise a few instances of downloading a totally new dataset. Take a look at it, see what you can define as a target, and determine how you can sort the data. Understanding what the dataset is saying has a learning curve, and this only comes with time. Read the dataset quickly, understand what it is about, and design an end-to-end datascience project around it.\n",
    "\n",
    "\"At the assignments there's often an import section which includes a solid outline for what could be / should be used within the assignment, will that happen in the MT?\" A: Yes, but there may be one or two missing\n",
    "\"Will the MT be in the same format at the assignments?\" You will be asked to do certain operations on a dataset, train the model, and interpolate the results. \"I mean, will it be in a Jupyter notebook setup?\" For the MT, you will get an empty jupyter notebook and then you will upload it to OWL.\n",
    "\n",
    "\"One varaible we cant account for is how long people take to finish assignments. I personally take forever. How long should the assignments take us?\" It depends on the individual, but for example, if you have not practiced GGPlot, then you won't be asked about GGPlot.\n",
    "\"Will the MT be similar in length and difficulty to the assignments?\" I dont know, we need to fit it within 2 - 3 hours. \"I am taking 7 or so hours per assignment..\" If you see something you've seen in the assignments it should be easier for you to do it again\n",
    "\n",
    "\"is it open book?\" Yes. But, only one screen allowed for the exam. \"Reference notes, assignments?\" Everything. Use ChatGPT.\n",
    "\n",
    "\"Any external monitors?\" No.\n",
    "\n",
    "\"Is this weeks class on the midterm?\" Yes, it will.\n",
    "\n",
    "On the tuesday after reading week, we will have a summary class where we can overview any other questions. \n",
    "\n",
    "The professor can see that we are putting in a lot of effort to keep on top of the class curriculum. So far, he knows how difficult it is, and he appreciates the hard work.\n",
    "\n",
    "\"Date and room of MT?\" You'll know later.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c0719",
   "metadata": {},
   "source": [
    "So far we have been doing lin. regression, classification, we should by now understand what is a cost function, regression, what is minimization, etc. etc. We really did not touch on the \"solvers\" which are the operations doing the heavy work behind the curtain. We will start with an overview, then the solvers, then different solvers, regularization...\n",
    "\n",
    "Topics covered in this section are important to the midterm. \n",
    "\n",
    "Slide 4:\n",
    "Can define a cost function as MSE. Training the model minimizes our loss / cost function. We use htheta as our \"base\" function. It can get more complex, but right now that is what we use. We have used the log transformation of that, and the polynomial.\n",
    "\n",
    "Slide 5:\n",
    "There is a nequation to find theta hat. It is the value that minimizes the cost function. For the thetas found, we get different from what is true because of the gaussian noise. The (X^TX) ^ -1 is concerning because it may not be invertable.\n",
    "\n",
    "Slide 6:\n",
    "Instead of that, we use thetahat = X^t y. That is the pseudoinverse, and it is always invertable.\n",
    "\n",
    "Slide 7:\n",
    "The complexity is how your floats scale with time ( ? ). Using the normal eq'n, we have order 3, but with SVD, we have order 3. The computational time scales linearly with the number of instances. What happens if we cannot fit our X within our machine, as the data is too large? What are our other options?\n",
    "\n",
    "Slide 8:\n",
    "Gradient Descent: We look at the gradient of the error and goes towards where gradient is 0 w.r.t theta. Our hyperparameter is a variable which tweaks our machine learning algorithm. If our learning rate (hyperparameter) is too small, then our time taken to model the data will be very large. If our learning rate is too high we may overshoot our minimum theta hat. To accellerate this, we can standardize our data, or \"Zeta scale\". If we do this, our gradient descent will go much faster. Instead of having to work between different parameters / if the data is \"unlike\" the error, then standardizing it would make it such that the data is \"speaking the same language\".\n",
    "\n",
    "Slide 9:\n",
    "What if there is only a local minimum?? \n",
    "\n",
    "Slide 10:\n",
    "We calculate the derivative of the cost function w.r.t the parameter. This can be written in the matrix form as a gradient. This is good and bad because it uses the WHOLE training data every time it is called on. That makes it very slow on large datasets, but, it is still much faster than the normal equation. Can we do bettter?\n",
    "\n",
    "Slide 11: \n",
    "To find the next theta, we take our hyperparameter, and we use that in tandem with the gradient of the cost function to determine how much we should change the theta. Using batch gradient descent, we see how the algorithm changes. At n = 0.02, it does not reach the data with it's line of best fit. With n = 0.5, it massively overshoots. We still want to use the faster algorithm, but we need to make sure we find the actual minimum.\n",
    "\n",
    "Slide 12:\n",
    "Stochastic is extremely good for this, at every step, it takes into account every data point you have. It picks a random instance of the training set, and it calculates the gradient based on that single instance. Given it does this randomly, it bounces hither and tither until it converges to the minimum. One issue with this, is that it does not find the best / the exact minimum. This is because you set a threshold where when you are within a certain threshhold of the minimum, it stops.\n",
    "\n",
    "Slide 13:\n",
    "To showcase the time saved, Stochastic Gradient Descent was 20x faster than batch gradient descent.\n",
    "\n",
    "Slide 14:\n",
    "Instead of the entire set, a mini batch takes small subsets of the data and creates gradients of those small random sets called Mini batches. It still fluctuates, but not as much as stochastic. This is better suited for GPU implementation as well.\n",
    "\n",
    "\"How small is the mini-batch?\" You play around with the mini batch. It is not large, but it should take in some variation of the data.\n",
    "\n",
    "Slide 15:\n",
    "The comparison chart tells us how the algorithms compare to one another. What does it mean by \"Requires scaling\"??\n",
    "\n",
    "Slide 16:\n",
    "Little thing about polynomial regression. One way to do polynomial estimation of fissures, is that you can add \"fissures\" of x as \"new fissures\". Here we would take columns of different fissures, and we can use a polynomial function from scikitlearn. You should know what the arguments are, and how to tweak it to include the parameters you'd need. It adds ALL possible interactions and powers unless you customize it. be aware of this transformation and how to work with it.\\\\\n",
    "\n",
    "Slide 17:\n",
    "Using that, we found this fit. The green line is extraordinarily high variance. How do we get rid of this while still using a high degree polynomial? With that, we are entering the subject of \"regularization\". We want to dampen the fluctuations of our high degree polynomials. By doing this, we brng into play bias. Now we are trying to be more adventurous with our models as opposed to restricting ourselves to less complex models to fight against overfitting. \n",
    "\n",
    "Slide 18:\n",
    "In the previous weeks tutorial he put a chunk explaining learning curve. This is the learning curve of the degree one polynomial. \n",
    "\n",
    "Slide 19:\n",
    "A higher degree model does markably better (notice the difference in RMSE between the slides. Is this model overfitting or underfitting? It is overfitting as it does better on the training set than the generalized data set. Underfitting is really bad, and it is described as having a high amount of RMSE. \n",
    "\n",
    "Slide 20:\n",
    "A models error on Generalization can be expressed as the sum of the three variables on screen. Typically, higher model complexity leads to a higher variance. \n",
    "\n",
    "Slide 21:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6ad45",
   "metadata": {},
   "source": [
    "Onto the secondary slides\n",
    "\n",
    "What features to we keep, what features do we not include?\n",
    "\n",
    "Imagine you have a dataset where you have the parameters frontage and depth of a house and you want to train a model on it to predict the price of the house. You have to determine if these features are dependant on each other, or independant. As a datascientist, you spend a lot of time determining what features to keep and what to drop, as to maximize the value of each feature.\n",
    "\n",
    "Fourier transformation takes us from time and brings us into frequency. Instead of how it changes with time, we determine how the frequency of occurance of the values.\n",
    "\n",
    "Feature Selection is a neat slide. Week6Pt.3.4'\n",
    "\n",
    "Will not do stepwise selection, he hates it. Better than Exhaustive, but there are assumptions that come with it. If the features are independant, it is fine. But, irl there are non-linearities and independant variables have dependancies still.\n",
    "\n",
    "None of these backwards / forwards are used IRL. We rely on our intellect to determine the best features.\n",
    "\n",
    "I zoned out for a long, long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b069b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c6ad30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f44b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec196b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8616cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9164f4d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0564001d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2521957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1429ed81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
