{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d47874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Week 5.1.\n",
    "We want to know what is training error, test error, and validation error. \n",
    "\n",
    "Before he explains that, he is drawing a cartoonish explanation:\n",
    "- We have a population, and we take a sample. From that sample, we keep a portion as \"Training\" and we keep a portion as \"Test\".\n",
    "- We do not touch the test until we finish training.\n",
    "- Today, we specify there is a subset of the training set we will keep that we will take to be our \"Validation\" set.\n",
    " \n",
    " So far we have discussed parameters, they were contained in \"x\". but, we actually keep a hyper-parameter (theta). \n",
    "        Theta tunes our solver to better go towards the minimum.\n",
    "    We check how good our models are through the \"Test Loss\"\n",
    "    \n",
    "\n",
    "Performance measures are as follow:\n",
    "    MSE (Mean squared error) - Residuals have a greater weight than regular points\n",
    "        Cannot be connected to the values we are predicting. It is not in the same units\n",
    "    RMSE (Root-Mean Squared Error)\n",
    "    MAE mean absolute error. This lets us find how far we are from the data set for each point. It does not help us determine\n",
    "        which points are outliers, it considers all lines to be equal.\n",
    "        MAE outputs your answer in the exact units we are originally considering. So it will tlel us \"you are off, on average,\n",
    "         about 20$\" if MAE is measuring $ and returns 20.\n",
    "    MRE.\n",
    "        This givs a percent. If we are off by MRE = 10, we are off on average by 10%.\n",
    "        \n",
    "    Another advantage of MSE / RMSE is that they are differentiable, but MRE and MAE are not.\n",
    "    \n",
    "    If we take outliers away, MRE and MAE are very nice. This was talked about earlier in the course.\n",
    "    \n",
    "    \n",
    "Test Error:\n",
    "ErrD is the error of a given test on a given test set.\n",
    "But we want Err, which is the error in general (?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db422e5b",
   "metadata": {},
   "source": [
    "# Week 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8ee485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Imagine you record the temptature of this room, and plot it, within each miniscule position, the temp. fluctuates. \n",
    "The variance of this should be measured by a good instrument, and it will output what the average is across a space which is\n",
    "greater than the miniscule position.\n",
    "    This is \"Irreducible error\" of the data. We want to take this out of our error of our model, as that is \"Noise\" of the data\n",
    "which is not related to our models\n",
    "\n",
    "    What is bias? Bias is the systematic difference of the model and the true value. If we know the data we are getting, and we \n",
    "know what functional form it follows, then we can use the same polynomial / exponential / function.\n",
    "In most cases, we do not know the functional form of the data. \n",
    "    If we choose a polynomial of degree 3, but the data is actually an exponential, that is a bias.\n",
    "    \n",
    "    There is an example of data whih is a non-linear x-y. The data comes with a bit of noise, so the data is read differently \n",
    "than what the data SHOULD be. The data given is actually across the dotted line, but we actually get it seperate from the line.\n",
    "    Here you can use spline fitting, exponential fitting, this, that, so long as it follows a non-linear curve. But, we \n",
    "are always using assumptions and trying our best.\n",
    "\n",
    "    First test is simple linear regression of polynomial degree 1. It gives us a blue line with residuals. Our predictions\n",
    "are quite different, as can be seen with the large residuals. Can we reduce these residuals by taking a fit which is closer\n",
    "to what we assume is the functional form / fit of the data? Yes. The next fit is a quadratic fit.\n",
    "    What about a polynomial of order 3? It fits nicely, even going through a few of the points. But, if we make it extremely\n",
    "complex, for example a polynomial of degree 10, then we fit exactly to the data, but we truly fit it \"to the noise\".\n",
    "\n",
    "    This is why we have training and testing! It allows us to check our predictions and see if we overfit (see, 10-deg-poly-fit)\n",
    "or if we have fit closer to the functional form of the data.\n",
    "\n",
    "    \n",
    "    \n",
    "    So, where is the sweetspot? We want to form a framework where we form a minimum bias and minimum variance. \n",
    "    \n",
    "Bias Variance Decomposition / Tradeoff\n",
    "    When bias goes down, variance goes up (this is why it is a tradeoff). The def of variance, as seen on the slide. \n",
    "    \n",
    "    Some people think with more complexity comes a better fitting model, but that could be completely wrong. \n",
    "A training loss of 0 could be correlational with an overfit. Having a training loss of 0.2 is to be expected with a proper model.\n",
    "    \n",
    "    Spaces of models, aka all models we can fit to the data, comes with a lower loss. If we take a hundred models and use \n",
    "them to fit, obviously we will fit every point of our train, but again, that could bring woes w.r.t our test.\n",
    "\n",
    "\n",
    "    In a graph of training vs test error, our training error can go down as we increase our model complexity, but our test error\n",
    "can be wildly off. So by looking at graphs of training error vs test error of multiple models, we can deduce which model leads\n",
    "to the least error in our test.\n",
    "\n",
    "    So if we plot a crazy amount of test vs. training errors. We can fit a model to the \"test error\" data set, and \n",
    "from that model we can find the best model complexity.\n",
    "\n",
    "Q: why do we not create greater / more complex splitting of the training and test set?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646f939",
   "metadata": {},
   "source": [
    "# Week 5.3\n",
    "I missed a few moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6356222e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (969763094.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_10776\\969763094.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Confidence interval of average test error. We can use a confidence interval to determine our confidence on our fit of our models.\n",
    "\n",
    "    How big should our test set be? Our test set needs to leave enough data for model fitting,\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5238967",
   "metadata": {},
   "source": [
    "# Week 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb69f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Choose a training set, divide the training set into k subsets. If we train on k-1, and test on the k, then we validate our \\nmodel as we train it. So if we split the training set into 4 partitions, we loop through training on first 3 then test on 4\\nthen train on last 3 and test on first... etc. We average the error across all \"folds\" of this data. \\n    The example shows six-fold validation. Six folds of data, and we iterate through them all. We see consistency between \\nall of the folds and trainings.\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Choose a training set, divide the training set into k subsets. If we train on k-1, and test on the k, then we validate our \n",
    "model as we train it. So if we split the training set into 4 partitions, we loop through training on first 3 then test on 4\n",
    "then train on last 3 and test on first... etc. We average the error across all \"folds\" of this data. \n",
    "    The example shows six-fold validation. Six folds of data, and we iterate through them all. We see consistency between \n",
    "all of the folds and trainings.\n",
    "\n",
    "\n",
    "#Week 5.6??? How did we skip 5.5?! (5.5 is the lab)\n",
    "\n",
    "I believe we are talking about having X number of data points in each fold. Leave One Out implies we test on each specific data\n",
    "point. could not mean that tho.\n",
    "\n",
    "In practical scenarios, we often take a \"K-Fold CV\" That means we test on many folds, but leave many out. If we have many folds\n",
    "and we leave only one fold out, and we iterate over all the folds with each taking their turn being the test, that will take a\n",
    "crazy amount of time.\n",
    "    \n",
    "    \"Leave One Out\" LOO has high variance. Every time you validate on a single point, there is a low likelihood that you will\n",
    "get good results from testing on it. Ca\n",
    "\n",
    "    Caveats / Considerations:\n",
    "    We can shuffle the data around in such a way that we can test in different ways. But, if there is a specified order of the \n",
    "data, we must not shuffle. Or, we could cut it into sections, and then train on a section of that dataset, and that section of \n",
    "the section of the data set should be at the beginning or the end of the section of the original data set.\n",
    "\n",
    "#Model Selection 5.8\n",
    "\n",
    "We want to take a good model, but for whatever reason that model is not the best model. So therefore, we take a model\n",
    "which has error of no more than one std error above the error of the best model. \n",
    "\n",
    "    In a test, we need to give reasoning for certain models. Justify, justify, justify, and try to stick within the bounds of\n",
    "1 std error of the error of the best model\n",
    "\n",
    "    Looking at the slide \"Test error on models\" we can take the second degree error, as it is within 1 std error of the \n",
    "error of degree 3 (which is the best model)\n",
    "\n",
    "    Choosing the model w lowest test error:\n",
    "        If you have 100 features, you select from 2^100 models.\n",
    "        \n",
    "        \n",
    "#Week 5.9\n",
    "    Many cases we want to transform our data. Imagine the case where you do some transformation of your data set, say\n",
    "you scale it in some way. Then, you pass your model to production, but the new data is not scaled as you scaled it. The beauty\n",
    "of using a pipeline in this case is that you can implement transformations within your model such that the data is transformed\n",
    "before the model predicts it. The transformations neccessary are dependant on many factors. We transformed data via log.\n",
    "\n",
    "    If you are doing regularization, ensure that you standardize your data! If you don't it makes it harder for your\n",
    "solver to find a minimum point. Or, lets say you have a fissure in the data that has values in [0,5].\n",
    "\n",
    "SIDE NOTE: Sklearn performance metrics and sklearn transformation\n",
    "    Different performance metrics are huuuge, auc scores, regression scores, root mean squared error, etc. etc.\n",
    "    \n",
    "    Creating a class allows us to define a transformation which we can implement into our \"pipeline\" (what is ran when our\n",
    "model is doing its predictions).\n",
    "\n",
    "Summary: That's it! The takeaway is the k-fold validation, a practical and standard way of validation. It will be done in the\n",
    "assignment. PCE!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fab88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
